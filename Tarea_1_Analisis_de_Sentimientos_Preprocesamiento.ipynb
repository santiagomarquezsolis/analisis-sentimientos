{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea 1. Analisis de Sentimientos. Preprocesamiento.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOIas4FHrNs7oKetDpmaxJB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/santiagomarquezsolis/analisis-sentimientos/blob/main/Tarea_1_Analisis_de_Sentimientos_Preprocesamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TAREA 1. PREPROCESAMIENTO**\n",
        "\n",
        "En este ejemplo, exploraremos como preprocesar tweets para hacer un análisis de sentimientos. Al final crearemos una función que realizará el preprocesamiento, pero antes vamos a ver los pasos que seguiremos en su codificación. Vamos a hacer uso del paquete NLTK que proporciona muchas utilidades interesantes y que nos da un comienzo rápido. \n",
        "\n"
      ],
      "metadata": {
        "id": "ZAZFBOax03g2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup**\n",
        "\n",
        "El paquete Natural Language Toolkit (NLTK), es una librería open-source de Python para el procesado del lenguaje natural. Tiene modulos para recolectar, manipular y procesar datos de Twitter de manera automática, dado que viene con algunos dataset de pruebas preinstalados. Estos datasets han sido manualmente anotados y sirven para establecer la línea base para el modelado rápido.\n",
        "\n",
        "Para hacer uso de un corpus tenemos que importar el paquete donde se encuentran localizados (nltk.corpus). Este modulo proporciona funciones para leer ficheros de corpues desde una amplia variedad de formatos, como para hacer uso de los corpus que vienen distribuidos con NLTK. Los corpus disponibles se encuentran en la siguiente URL:\n",
        "\n",
        "https://www.nltk.org/api/nltk.corpus.html\n",
        "\n"
      ],
      "metadata": {
        "id": "T7lOOvH21jwA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HM5zvFsFQUE6"
      },
      "outputs": [],
      "source": [
        "import nltk                                # Python library for NLP\n",
        "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
        "import matplotlib.pyplot as plt            # library for visualization\n",
        "import random                              # pseudo-random number generator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Acerca del conjunto de datos de Twitter**\n",
        "\n",
        "El dataset de ejemplo de Twitter está separado en tweets positivos y negativos. Contiene 5000 tweets positivos y otros tantos negativos. La coincidencia entre estas dos clases no es casual, ya que la intención es tener un conjunto de datos balenceados, aunque esto no refleja la distribución real de clases positivas y negativas en el stream de datos en real de Twitter. Los datasets balanceados simplifican el diseño de los algoritmos, sin embargo, tenemos que ser conscientes que el balanceo de clases no es lo habitual. \n",
        "\n",
        "Podemos descargar el dataset haciendo lo siguiente (el \"twitter_samples\" es el id del dataset, en la URL anteriormente indicada se indica este):"
      ],
      "metadata": {
        "id": "XXrjcjzz-J6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloads sample twitter dataset.\n",
        "nltk.download('twitter_samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKWxVFYj-KyE",
        "outputId": "1b2da839-0d0b-44a8-991d-6a26043deae0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos cargar los campos de texto de todos los tweets positivos y negativos usando el método `strings()` del modo siguiente:"
      ],
      "metadata": {
        "id": "YalJcPlBCDvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select the set of positive and negative tweets\n",
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDHv4Rs6COdI",
        "outputId": "a6a1bf41-5370-4ede-c8f7-0362ec0480c5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method count of list object at 0x7fe0136219b0>\n"
          ]
        }
      ]
    }
  ]
}